import sys 
from pyspark import SparkContext
if __name__ == "__main__": 
	if len(sys.argv) < 2:
		print >> sys.stderr, "Usage: Page_Rank_Spark_ML.py<file>" 
		exit(-1) 
	sc = SparkContext()
	def computeContribs(neighbors, rank):
		for neighbor in neighbors: yield(neighbor, rank / len(neighbors))
	links = sc.textFile(sys.argv[1]) .map(lambda line: line.split()).map(lambda pages: (pages[0],pages[1])).distinct().groupByKey().persist()
	ranks=links.map(lambda (page,neighbors): (page,1.0))
	for x in xrange(10):
		contribs=links.join(ranks).flatMap(lambda (page,(neighbors,rank)): computeContribs(neighbors,rank)) 
		ranks=contribs.reduceByKey(lambda v1,v2: v1+v2).map(lambda (page,contrib): (page,contrib * 0.85 + 0.15))
	for rank in ranks.collect(): print rank
	sc.stop()
		


    